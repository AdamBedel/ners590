{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Lecture Notes 9/16/2024 - 9/18/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training / Validation / Testing\n",
    "- validation provides on-the-fly model validation during training process to imporve model parameters\n",
    "    - wouldn't want to train the model for multiple days then find out it's shit\n",
    "    - prevents overfitting\n",
    "- Reimplement k-fold cross-validation for small datasets\n",
    "- Sigmoid function $ \\frac 1 {(1-e^{-x})} $ kinda like a step, its derivative is $ x(1-x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning\n",
    "- Feedforward Neural Network\n",
    "    - Many names, Mulitlayer Perceptron, Fully-connected NN, Dense Network\n",
    "- Universal Approximation Theorem - neural networks can always approximate any continuous function to arbitrary precision given at least 1 hidden layer\n",
    "- Multiple Layers\n",
    "    - Input / feature layer (X-array) [2d-array, rows are samples, columns are features]\n",
    "    - Hidden layer(s)\n",
    "        - Use activation functions (and weights (W-matrix) and biases (b-vector) to transform X) to transform X into Y\n",
    "        - Use either rectified linear units (ReLU): $ g(z) = max(0,z) $ or Sigmoid (only between 0-1): $ g(z) = \\frac 1 {1+e^{-z}} $ or tanh (only between -1 - -1)\n",
    "            - Necessarily have continuous, easy-to-derive derivatives\n",
    "    - Output layer (Y-array)\n",
    "        - Also has it's own activation function (one last set of weights and biases)\n",
    "        - Activation function might take different forms given the output type desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers and Backpropagation\n",
    "- Cost Function (called loss function): \n",
    "    - Regression: minimize mean squared error or mean absolute error\n",
    "    - Classification: maximize Accuracy = # correct / # of predictions\n",
    "- We use gradient descent to iterate (walk in the direction of the gradient to find the minima)\n",
    "- Learning rate $ \\alpha $ is the step size at each iteration the optimizer uses while moving toward a minimum of the loss function\n",
    "    - Use Adam as the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward and backpropagation\n",
    "- Forward pass is simple and intuitive, use x --> y\n",
    "- Backward pass involves estimating derivatives, but those derivatives multiplied by a step size (the learning rate) gives you new weights to try\n",
    "- one forward pass plus one backward pass is called an epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs and Batch Size (Welcome to the world of hyperparameters)\n",
    "- Number of epochs is a hyerparameter (generally, set epochs > 15)\n",
    "- Batch size is a hyperparameter (how many samples to use before updating network parameters, usually a power of 2)\n",
    "    - Trade off between speed and memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks- diagnostic objects outputted during training\n",
    "- CSVLogger: write a csv to monitor your metric\n",
    "- ModelCheckpoint: periodically save your NN\n",
    "- EarlyStopping: Stop training when a monitored metric has stopped improving\n",
    "- ReduceLROnPlateau: Reduce learning rate when a metric has stopped improving (maybe not useful for Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning and Overfitting\n",
    "- Deeper the model, more likely to overfit\n",
    "    - Can be prevented with dropout layers, which we'll discuss more later\n",
    "- Depth can make width unneccesary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 9/18/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never import keras and tensorflow, tensorflow already includes keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physics-informed Neural Networks (PINN)\n",
    "- Class of neural networks to solve PDEs using physics laws\n",
    "- Use BC, IC, and physics laws to evaluate PDE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolmogorov-Arnold Networks\n",
    "- Remember that FNN is built on Universal Approximation Theorem"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
