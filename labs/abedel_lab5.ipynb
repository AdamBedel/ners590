{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib #for parallelization\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Scikit-optimise\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.utils import use_named_args\n",
    "# Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# For grid tuning\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to be tuned\n",
    "- num Layers\n",
    "- nodes / layer\n",
    "- learning rate\n",
    "- batch size\n",
    "- dropout rate\n",
    "- num epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evol search, random search, manual testing, bayesian optimization, grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to optimize the following hyperparameters:\n",
    "> num_layers, nodes_per_layer, learning_rate, batch_size, dropout_rate, num_epochs = []\n",
    "\n",
    "\n",
    "Perform some manual testing to find limits of an acceptable runtime for the very largest cases \n",
    "> while(max_hyperparameter_values == unknown)\n",
    "    > large_guesses = [large_num_layers, large_nodes_per_layer, -, large_batch_size, -, large_num_epochs]\n",
    "    > manual_testing(run_FNN(large_guesses))\n",
    "    > update(max_hyperparameter_values)\n",
    "\n",
    "For two of the hyperparameters, dropout_rate and learning_rate, we guess some standard values for their max size\n",
    "> max_dropout_rate = 0.5\n",
    "> max_learning_rate = 1e-3\n",
    "\n",
    "To find minimum hyperparameter values, we can use some random guesses loosely based on how many inputs we have.\n",
    "Below is some example values \n",
    "> min_num_layers = 3\n",
    "> min_nodes_per_layer = max(num_FNN_inputs, 3)\n",
    "> min_learning_rate = 1e-4\n",
    "> min_batch_size = 1\n",
    "> min_dropout_rate = 0.01\n",
    "> min_num_epochs = 10\n",
    "\n",
    "We now know the range of hyperparameter values to sweep over with our optimization. These max values only\n",
    "are constrained by runtime, so this tells us nothing about performance of our FNN.\n",
    "> range_hyperparameters = [min_num_layers, max_num_layers], [min_nodes_per_layer, max_nodes_per_layer], ...\n",
    "\n",
    "Note that at this step we should take note that some values are discrete and some are continuous. Our genetic algorithm \n",
    "should be able to handle either.\n",
    "\n",
    "For the actual hyperparameter tuning, we'll use a genetic algorithm since they're easy to parallelize and understand. \n",
    "We can setup a cost function based off the performance of our FNN.\n",
    "> def cost(FNN)\n",
    "    > return R^2 of FNN from testing values\n",
    "\n",
    "> def run_multithreading():\n",
    "    > for i <= num_threads\n",
    "        > random_guess_i = some_values_in(range_hyperparameters)\n",
    "        > thread_i = thread_setup(target=cost(), parameters = FNN(random_guess))\n",
    "        > thread_i.start()\n",
    "    > join_threads()\n",
    "    return costs\n",
    "\n",
    "> if name == '__main__':\n",
    "    > num_threads = hopefully_a_bunch\n",
    "    > hyperparameter_suite = random_values_in(range_hyperparameters)\n",
    "    > run_multithreading()\n",
    "    > for i <= num_generations\n",
    "        > kill_bad_guesses(hyperparameter_suite)\n",
    "        > mutate_good_guesses(hyperparameter_suite)\n",
    "        > run_multithreading((hyperparameter_suite)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Set 2 (a)\n",
    "learning_rate = [1e-4, 5e-4, 7.5e-4, 1e-3]\n",
    "num_layers = [3, 4, 5, 6]\n",
    "num_nodes = [50, 100, 150, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Set 2 (b)\n",
    "dim_learning_rate = Real(low=1e-4, high=1e-3, name='learning_rate')\n",
    "dim_num_layers = Integer(low=3, high=8, name='num_layers')                  \n",
    "dim_num_nodes = Categorical(categories=(25, 50, 100, 150, 200, 250, 300), \\\n",
    "                            name='num_nodes')\n",
    "\n",
    "init_guess = [5e-4, 4, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Set 2 (c)\n",
    "learning_rate_random = np.random.uniform(1e-4, 1e-3, size=4)\n",
    "num_layers_random = np.random.randint(3, 9, size=4)     # rand over [3,8]\n",
    "num_nodes_random = np.random.choice([25, 50, 100, 150, 200, 250, 300], size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Set 2 (d)\n",
    "#-----------------------\n",
    "# Data preprocessing\n",
    "#-----------------------\n",
    "\n",
    "#load the x,y data and convert to numpy array\n",
    "xurl='https://raw.githubusercontent.com/aims-umich/ners590data/main/crx.csv'\n",
    "yurl='https://raw.githubusercontent.com/aims-umich/ners590data/main/powery.csv'\n",
    "xdata=pd.read_csv(xurl).values\n",
    "ydata=pd.read_csv(yurl).values\n",
    "\n",
    "# split into training/testing sets\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(xdata, ydata, test_size=0.2, random_state=42)\n",
    "\n",
    "#create min-max scaled data\n",
    "xscaler = MinMaxScaler()\n",
    "yscaler = MinMaxScaler()\n",
    "Xtrain=xscaler.fit_transform(xtrain)\n",
    "Xtest=xscaler.transform(xtest)\n",
    "Ytrain=yscaler.fit_transform(ytrain)\n",
    "Ytest=yscaler.transform(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(args):\n",
    "    lr, nl, nn = args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ners590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
