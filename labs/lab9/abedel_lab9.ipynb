{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Lab 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 20:12:49.751185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-28 20:12:49.903630: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-28 20:12:49.950122: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-28 20:12:50.208709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-28 20:13:16.377695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/adambedel/miniconda3/envs/ners590/lib/python3.12/site-packages/numpy/core/getlimits.py:542: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf #you can tf.keras to access most of the needed items\n",
    "from collections import deque #get to know this package, it is amazing to build your replay buffer :)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001 #use mse loss, adam optimizer, relu and linear activations\n",
    "gamma = 0.99 #reward discount factor\n",
    "epsilon_init = 1.0 #start with full exploration\n",
    "epsilon_min = 0.01 #towards the end you are only 10\\% likely to explore, otherwise exploit\n",
    "epsilon_decay = 0.995 #decay after every episode\n",
    "batch_size = 64 #for gradient descent\n",
    "memory_size = 2000 #size of the replay buffer\n",
    "target_update_freq = 10 #parameter C in slides, when to copy the primary to the target network\n",
    "num_episodes = 500 #total episodes for training\n",
    "num_layers = [64,64] #these are hidden layers only for the Q/primary network\n",
    "num_epochs = 3 #only fit the primary/Q network for 3 epochs to finish fast\n",
    "\n",
    "# Create the environment (for testing )\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0] #this gives you indication of the state space\n",
    "action_size = env.action_space.n #this gives you indication of the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize replay memory (ReplayBuffer) to store past experiences\n",
    "replayBuffer = deque()\n",
    "\n",
    "# 2. Initialize Q−network (a.k.a Primary Network) with random weights\n",
    "primaryQN = tf.keras.Sequential()\n",
    "primaryQN.add(tf.keras.Input(shape=(4,)))\n",
    "primaryQN.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "primaryQN.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "primaryQN.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "\n",
    "# 3. Initialize target Q−network (Target−Q−network) with same weights as Q−network\n",
    "targetQN = tf.keras.Sequential()\n",
    "targetQN.add(tf.keras.Input(shape=(4,)))\n",
    "targetQN.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "targetQN.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "targetQN.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "targetQN.set_weights(primaryQN.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = epsilon_init\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    for j in range(500):\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = targetQN.predict(state)\n",
    "\n",
    "        nextState, reward, done, info = env.step(action)\n",
    "        replayBuffer.append([state, action, reward, nextState])\n",
    "\n",
    "        # 14. If replay memory contains enough samples:\n",
    "        if len(replayBuffer) >= batch_size:\n",
    "            # 15. Sample a batch of transitions (state , action , reward, next state) from replay memory\n",
    "\n",
    "            # 16. For each transition in the mini−batch: #this is the most tricky part!\n",
    "                # 17. If next state is terminal:\n",
    "                    # 18. Set target = reward\n",
    "                # 19. Otherwise:\n",
    "                    # 20. Set target = reward + discount factor ∗ max(Target−Q−network(next state))\n",
    "                # 21. Perform gradient descent step to minimize loss :\n",
    "                    # loss = (target − Q−network(state, action))ˆ2\n",
    "                #22. Update Q−network weights using gradient descent\n",
    "                \n",
    "        state = nextState\n",
    "\n",
    "        if j % target_update_freq == 0:\n",
    "              targetQN.set_weights(primaryQN.get_weights())\n",
    "\n",
    "        epsilon = epsilon * epsilon_decay\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ners590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
