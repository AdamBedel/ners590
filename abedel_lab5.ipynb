{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 10:56:12.413830: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-02 10:56:12.423771: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-02 10:56:12.445254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 10:56:12.474531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 10:56:12.483372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 10:56:12.512307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 10:56:16.368873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/abedel/miniconda3/envs/ners590/lib/python3.12/site-packages/numpy/core/getlimits.py:542: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "# Basic Packages\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib #for parallelization\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Scikit-optimise\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.utils import use_named_args\n",
    "# Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# For grid tuning\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to be tuned\n",
    "- num Layers\n",
    "- nodes / layer\n",
    "- learning rate\n",
    "- batch size\n",
    "- dropout rate\n",
    "- num epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evol search, random search, manual testing, bayesian optimization, grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to optimize the following hyperparameters:\n",
    "> num_layers, nodes_per_layer, learning_rate, batch_size, dropout_rate, num_epochs = []\n",
    "\n",
    "\n",
    "Perform some manual testing to find limits of an acceptable runtime for the very largest cases \n",
    "> while(max_hyperparameter_values == unknown)\n",
    "    > large_guesses = [large_num_layers, large_nodes_per_layer, -, large_batch_size, -, large_num_epochs]\n",
    "    > manual_testing(run_FNN(large_guesses))\n",
    "    > update(max_hyperparameter_values)\n",
    "\n",
    "For two of the hyperparameters, dropout_rate and learning_rate, we guess some standard values for their max size\n",
    "> max_dropout_rate = 0.5\n",
    "> max_learning_rate = 1e-3\n",
    "\n",
    "To find minimum hyperparameter values, we can use some random guesses loosely based on how many inputs we have.\n",
    "Below is some example values \n",
    "> min_num_layers = 3\n",
    "> min_nodes_per_layer = max(num_FNN_inputs, 3)\n",
    "> min_learning_rate = 1e-4\n",
    "> min_batch_size = 1\n",
    "> min_dropout_rate = 0.01\n",
    "> min_num_epochs = 10\n",
    "\n",
    "We now know the range of hyperparameter values to sweep over with our optimization. These max values only\n",
    "are constrained by runtime, so this tells us nothing about performance of our FNN.\n",
    "> range_hyperparameters = [min_num_layers, max_num_layers], [min_nodes_per_layer, max_nodes_per_layer], ...\n",
    "\n",
    "Note that at this step we should take note that some values are discrete and some are continuous. Our genetic algorithm \n",
    "should be able to handle either.\n",
    "\n",
    "For the actual hyperparameter tuning, we'll use a genetic algorithm since they're easy to parallelize and understand. \n",
    "We can setup a cost function based off the performance of our FNN.\n",
    "> def cost(FNN)\n",
    "    > return R^2 of FNN from testing values\n",
    "\n",
    "> def run_multithreading():\n",
    "    > for i <= num_threads\n",
    "        > random_guess_i = some_values_in(range_hyperparameters)\n",
    "        > thread_i = thread_setup(target=cost(), parameters = FNN(random_guess))\n",
    "        > thread_i.start()\n",
    "    > join_threads()\n",
    "    return costs\n",
    "\n",
    "> if name == '__main__':\n",
    "    > num_threads = hopefully_a_bunch\n",
    "    > hyperparameter_suite = random_values_in(range_hyperparameters)\n",
    "    > run_multithreading()\n",
    "    > for i <= num_generations\n",
    "        > kill_bad_guesses(hyperparameter_suite)\n",
    "        > mutate_good_guesses(hyperparameter_suite)\n",
    "        > run_multithreading((hyperparameter_suite)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ners590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
